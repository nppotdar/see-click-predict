# -*- coding: utf-8 -*-
"""
Created on Sat Nov 09 14:35:42 2013

@author: Standard User
"""

''' 
DATA DICTIONARY
id - a randomly assigned id
latitude - the latitude of the issue
longitude - the longitude of the issue
summary - a short text title
description - a longer text explanation
num_votes - the number of user-generated votes
num_comments - the number of user-generated comments
num_views - the number of views
source - a categorical variable indicating where the issue was created
created_time - the time the issue originated
tag_type - a categorical variable (assigned automatically) of the type of issue
'''

''' Libraries and globals '''
import numpy as np
import csv
import math
import time
from sklearn.tree import DecisionTreeRegressor
from sklearn import preprocessing
from sklearn.feature_extraction import DictVectorizer
from sklearn.ensemble import AdaBoostRegressor
from sklearn import cross_validation
from sklearn.metrics import make_scorer

''' Functions and classes '''
def loss(p_num_votes, p_num_comments, p_num_views, a_num_votes, a_num_comments, a_num_views):
    # check if shape is the same
    n = p_num_votes.size + p_num_comments.size + p_num_views.size
    log_p_num_votes = np.log(p_num_votes + 1)
    log_p_num_comments = np.log(p_num_comments + 1)
    log_p_num_views = np.log(p_num_views + 1)
    log_a_num_votes = np.log(a_num_votes + 1)
    log_a_num_comments = np.log(a_num_comments + 1)
    log_a_num_views = np.log(a_num_views + 1)
    sum_num_votes = np.sum(np.square(log_p_num_votes - log_a_num_votes))
    sum_num_comments = np.sum(np.square(log_p_num_comments - log_a_num_comments))
    sum_num_views = np.sum(np.square(log_p_num_views - log_a_num_views))
    rmsle = np.sqrt((sum_num_votes + sum_num_comments + sum_num_views) / n)
    return rmsle

def scorer_loss(y_predicted, y):
    rmsle = loss(y_predicted[:, 0], y_predicted[:, 1], y_predicted[:, 2], y[:, 0], y[:, 1], y[:, 2])
    return rmsle

##def scorer_rmsle(clf, X, y):
    ##clf.fit(X, y)
    ##y_predicted = clf.predict(X)
    ##rmsle = loss(y_predicted[:, 0], y_predicted[:, 1], y_predicted[:, 2], y[:, 0], y[:, 1], y[:, 2])
    ##return rmsle
    

''' Load data '''
# load training data into dict
data = []
file_name = "../files/train.csv"
reader = csv.DictReader(open(file_name, 'rb'), delimiter=',', quotechar='"')
for row in reader:
    data.append(row)
rng = np.random.RandomState(1)
# convert appropriate keys from string to appropriate type
for sub in data:
    for key in sub:
        if key == 'id' or key == 'num_votes' or key == 'num_comments' or key == 'num_views':
            sub[key] = int(sub[key])
        elif key == 'latitude' or key =='longitude':
            sub[key] = float(sub[key])
        elif key == 'created_time':
            sub[key] = time.mktime(time.strptime(sub[key], "%Y-%m-%d %H:%M:%S"))    # make time into datetime (float)
        elif key == 'summary':
            sub[key] = len(sub[key])
        elif key == 'description':
            sub[key] = len(sub[key])
print "Training data loaded successfully!"

''' Pre-process training data '''
# select which variables to fit the model on
X_vars = ['source', 'tag_type']
Y_vars = ['num_votes', 'num_comments', 'num_views']
X = []
Y = []

for d in data:
    subdict_X = dict([(i, d[i]) for i in X_vars if i in d])
    subdict_Y = dict([(i, d[i]) for i in Y_vars if i in d])
    subdict_X["loc"] = "%s%s" % ( str( math.ceil(abs( float(d['latitude'])) ) ), str( math.ceil(abs(float(d['longitude']))) ) ) 

    X.append(subdict_X)
    #X.append(subdict_loc)
    Y.append(subdict_Y)

# encode categorical variables in X
vec_X = DictVectorizer()
vec_Y = DictVectorizer()
X_transformed = vec_X.fit_transform(X)
X_encoded = X_transformed.toarray()
# convert y into proper numpy array
Y_transformed = vec_Y.fit_transform(Y)
Y_encoded = Y_transformed.toarray()
# separate training/testing data
X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X_encoded, Y_encoded, test_size=0.5)
# remove unneeded dimension
Y_train = np.squeeze(Y_train)
Y_test = np.squeeze(Y_test)

print len(X_encoded)

output_csv = csv.writer( open("./Xencoded.csv","wb"), X_encoded, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)

output_csv.writerow(vec_X.get_feature_names())
for row in X_encoded:
    output_csv.writerow(row)

print 'DONE'


#''' Begin building the model on training data'''
## decision tree model
#depth = 12 # 12 is optimal for num_votes
#print "max_depth =", depth
#clf = DecisionTreeRegressor(max_depth=depth)
#clf.fit(X_train, y_train)
#
#y_train_predicted = clf.predict(X_train)
#y_test_predicted = clf.predict(X_test)
#train_loss = loss(y_train_predicted[:, 0], y_train_predicted[:, 1],  y_train_predicted[:, 2],  y_train[:, 0], y_train[:, 1],  y_train[:, 2])
#test_loss = loss(y_test_predicted[:, 0], y_test_predicted[:, 1], y_test_predicted[:, 2], y_test[:, 0], y_test[:, 1], y_test[:, 2])
#print "Train loss =", train_loss
#print "Test loss =", test_loss
#
#
######
#raw_input("New cv method")
#clf_new = DecisionTreeRegressor(max_depth=depth)
#custom_scorer = make_scorer(scorer_loss, greater_is_better=False)
#scores = cross_validation.cross_val_score(clf_new, X_encoded, y_encoded, cv=5, scoring=custom_scorer)
#print scores
#clf_new.fit(X_encoded, y_encoded)
#print scorer_loss(clf_new.predict(X_encoded), y_encoded)
#raw_input("/New cv method")
######
#
#''' Load test data '''
## load test data into dict
#data = []
#file_name = "C:/Users/Standard User/Downloads/test.csv"
#reader = csv.DictReader(open(file_name, 'rb'), delimiter=',', quotechar='"')
#for row in reader:
#    data.append(row)
#
## convert appropriate keys from string to appropriate type
#for sub in data:
#    for key in sub:
#        if key == 'id' or key == 'num_votes' or key == 'num_comments' or key == 'num_views':
#            sub[key] = int(sub[key])
#        elif key == 'latitude' or key =='longitude':
#            sub[key] = float(sub[key])
#        elif key == 'created_time':
#            sub[key] = time.mktime(time.strptime(sub[key], "%Y-%m-%d %H:%M:%S"))
#        elif key == 'summary':
#            sub[key] = len(sub[key])
#        elif key == 'description':
#            sub[key] = len(sub[key])
#
#print "Testing data loaded successfully!"
#
#''' Pre-process test data '''
## select which variables to fit the model on
#X = []
#for d in data:
#    subdict_X = dict([(i, d[i]) for i in X_vars if i in d])
#    X.append(subdict_X)
#    
## encode categorical variables in X
#vec = DictVectorizer()
#X_encoded = vec_X.transform(X).toarray()
#
#''' Fit model to test data '''
#y_predicted = clf.predict(X_encoded)
#print "Testing model fit!"
#
#''' Write submission file for test data '''
#raw_input("Press enter to write submission file")
#id_list = [d['id'] for d in data]
#id_list = np.array(id_list)[:, np.newaxis]
#header_text = "id,num_views,num_votes,num_comments"
#submission_fname = "C:/Users/Standard User/sampleSubmissions111.csv"
#output_array = np.hstack((id_list, y_predicted))
#np.savetxt(submission_fname, output_array, delimiter=",", header=header_text, fmt="%f")
#print "Submission file written to '", submission_fname, "'"
